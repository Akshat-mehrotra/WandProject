{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a17760-e6d0-423b-91c8-ce62090b64b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From g:\\Python3.11.1\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, Normalization, BatchNormalization, LayerNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013edb5c-ae83-4c41-bd48-06c6cc447cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData3x(file):\n",
    "    value = []\n",
    "    temp = []\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if (line[0] != '('):\n",
    "                if temp: \n",
    "                    value.append(np.array(temp))\n",
    "                temp = []\n",
    "            else:\n",
    "                data = line[1:-2].split(',')\n",
    "                temp.append([float(data[0]), float(data[1]), float(data[2])])\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8c4e70-c227-4c3b-a98e-7abc5223d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 -> 264\n",
      "S2 -> 183\n",
      "S3 -> 202\n"
     ]
    }
   ],
   "source": [
    "wL = readData3x(\"hundred_right_lines/ControllerAngularVelocity.txt\")\n",
    "vL = readData3x(\"hundred_right_lines/ControllerVelocity.txt\")\n",
    "\n",
    "wI = readData3x(\"hundredInfinity/ControllerAngularVelocity.txt\")\n",
    "vI = readData3x(\"hundredInfinity/ControllerVelocity.txt\")\n",
    "\n",
    "wC = readData3x(\"hundredrightcircles/ControllerAngularVelocity.txt\")\n",
    "vC = readData3x(\"hundredrightcircles/ControllerVelocity.txt\")\n",
    "\n",
    "wS1 = readData3x(\"spell1CWC/ControllerAngularVelocity.txt\")\n",
    "vS1 = readData3x(\"spell1CWC/ControllerVelocity.txt\")\n",
    "\n",
    "wS2 = readData3x(\"spell2line/ControllerAngularVelocity.txt\")\n",
    "vS2 = readData3x(\"spell2line/ControllerVelocity.txt\")\n",
    "\n",
    "wS3 = readData3x(\"spell3/ControllerAngularVelocity.txt\")\n",
    "vS3 = readData3x(\"spell3/ControllerVelocity.txt\")\n",
    "            # This order of w first then v is important\n",
    "# DataSet = {'line': [wL, vL], 'infinity': [wI, vI], 'circle': [wC, vC]}\n",
    "# lenghtestlist = [wL, wI, wC]\n",
    "\n",
    "DataSet = {'S1': [wS1, vS1], 'S2': [wS2, vS2], 'S3': [wS3, vS3]}\n",
    "lenghtestlist = [v[0] for v in DataSet.values()]\n",
    "for label, data in DataSet.items():\n",
    "    print(label, \"->\", len(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba6032ba-01fd-4732-95fb-e1a811324df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of each 2D sample 96\n",
      "minimum samples per category are 183\n"
     ]
    }
   ],
   "source": [
    "longestLengths = []\n",
    "for type in lenghtestlist:\n",
    "    longestLengths.append(max([x.shape[0] for x in type]))\n",
    "\n",
    "lengthFinal = max(longestLengths)\n",
    "print(\"length of each 2D sample\", lengthFinal)\n",
    "\n",
    "minSampleSize = min([len(i[0]) for i in DataSet.values()])\n",
    "print(\"minimum samples per category are\", minSampleSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255929a3-aa35-4d27-b623-5d14565bdefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 (264, 96, 3)\n",
      "S1 (264, 96, 3)\n",
      "dstacked: (264, 96, 6)\n",
      "chopped to min sample size: (183, 96, 6)\n",
      "\n",
      "S2 (183, 96, 3)\n",
      "S2 (183, 96, 3)\n",
      "dstacked: (183, 96, 6)\n",
      "chopped to min sample size: (183, 96, 6)\n",
      "\n",
      "S3 (202, 96, 3)\n",
      "S3 (202, 96, 3)\n",
      "dstacked: (202, 96, 6)\n",
      "chopped to min sample size: (183, 96, 6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDataSet = []\n",
    "y = []\n",
    "for label, dataType in DataSet.items():\n",
    "    dataTypeMatriciesLists = [] # list of data per datatypes for angualrData, velocity data\n",
    "    for sampleList in dataType: # loops through angularV, linearV, accelerations etcs...\n",
    "        newDataMatrixList = [] # list of all sampels for that dataType\n",
    "        for sample in sampleList:\n",
    "            # zero padding on both sides\n",
    "            zerosLeft = (lengthFinal - sample.shape[0]) // 2\n",
    "            zerosRight = (lengthFinal - sample.shape[0] - zerosLeft)\n",
    "            # zero pad right side\n",
    "            # zerosLeft = 0\n",
    "            # zerosRight = lengthFinal - sample.shape[0]\n",
    "            # zero padding only on right size\n",
    "            newDataMatrix = np.pad(sample, ((zerosLeft, zerosRight), (0, 0)), 'constant')\n",
    "            # print(sample.shape, '->', newDataMatrix.shape)\n",
    "            \n",
    "            newDataMatrixList.append(newDataMatrix)\n",
    "        dataTypeMatriciesLists.append(np.array(newDataMatrixList))\n",
    "        \n",
    "    [print(label,i.shape) for i in dataTypeMatriciesLists]\n",
    "    \n",
    "    combinedData = np.dstack(dataTypeMatriciesLists)\n",
    "    print(\"dstacked:\", combinedData.shape)\n",
    "    \n",
    "    combinedData = combinedData[:minSampleSize, ] # chop of so all labels are balanced\n",
    "    \n",
    "    print(\"chopped to min sample size:\", combinedData.shape, end=\"\\n\\n\")\n",
    "    newDataSet.append(combinedData)\n",
    "    y.append(np.full((combinedData.shape[0], 1), label))\n",
    "    # print(\"label dim:\", y[-1].shape, end=\"\\n\\n\")\n",
    "    \n",
    "newDataSet = np.vstack(newDataSet)\n",
    "DataSety = np.vstack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e806e9c-0c88-4dff-8e5d-494cd9e88691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((549, 96, 6), (549, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDataSet.shape, DataSety.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04ce664-492b-484e-a10e-c4bc31d8145b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Python3.11.1\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('S1', '->', 0), ('S2', '->', 1), ('S3', '->', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdScaler = StandardScaler()\n",
    "mmScaler = MinMaxScaler()\n",
    "stdX = np.array([stdScaler.fit_transform(newDataSet[x,]) for x in range(newDataSet.shape[0])])\n",
    "# mmX =  np.array([mmScaler.fit_transform(newDataSet[x,]) for x in range(newDataSet.shape[0])])\n",
    "normX = np.array([normalize(newDataSet[x,], norm='l1') for x in range(newDataSet.shape[0])])\n",
    "\n",
    "label = LabelEncoder()\n",
    "DataSety = label.fit_transform(DataSety)\n",
    "\n",
    "[(label.classes_[i], '->', i) for i in range(len(label.classes_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7328054-b5f0-415a-8c4a-cb0c7b22e3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c781327-aa13-4070-aa3e-aa4ed4f6a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (494, 96, 6) test shape (55, 96, 6)\n",
      "train2D shape: (494, 576) test2D shape (55, 576)\n",
      "\n",
      "Y train shape: (494,) Y test shape (55,)\n",
      "one hot train shape:-> (494, 3) one hot test shape:-> (55, 3)\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(newDataSet, DataSety, test_size = 0.1, random_state = 30, stratify = DataSety)\n",
    "trainX2D = trainX.reshape((trainX.shape[0], trainX.shape[1]*trainX.shape[2])) # (# of samples, xyz for w and v for each timestamp )\n",
    "testX2D  = testX.reshape((testX.shape[0], testX.shape[1]*testX.shape[2]))\n",
    "\n",
    "print(\"train shape:\", trainX.shape, \"test shape\", testX.shape)\n",
    "print(\"train2D shape:\", trainX2D.shape, \"test2D shape\", testX2D.shape)\n",
    "\n",
    "print()\n",
    "print(\"Y train shape:\", trainy.shape, \"Y test shape\", testy.shape)\n",
    "\n",
    "# one hot encode y\n",
    "trainy1h = to_categorical(trainy)\n",
    "testy1h = to_categorical(testy)\n",
    "print(\"one hot train shape:->\", trainy1h.shape, \"one hot test shape:->\", testy1h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf38210b-3a75-4a8b-8948-920af72794e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy1h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cff78652-ec6a-4c66-b266-469f287ee6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy1h.shape[1]\n",
    "modelNN = Sequential()\n",
    "modelNN.add(Normalization(input_shape=(n_timesteps,n_features), name='input'))\n",
    "modelNN.add(LSTM(100))\n",
    "# modelNN.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "# modelNN.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "modelNN.add(Dropout(0.5))\n",
    "# modelNN.add(MaxPooling1D(pool_size=2))\n",
    "# modelNN.add(Flatten())\n",
    "modelNN.add(Dense(100, activation='relu'))\n",
    "modelNN.add(Dense(n_outputs, activation='softmax', name='output'))\n",
    "modelNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d66ddc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Normalization)       (None, 96, 6)             13        \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               42800     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53216 (207.88 KB)\n",
      "Trainable params: 53203 (207.82 KB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7c236-63a0-4ff7-b58f-19862db21559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 71ms/step - loss: 3.9393e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "modelNN.fit(trainX, trainy1h, epochs=50, batch_size=32, verbose=1)\n",
    "loss_NN, trainacc_NN = modelNN.evaluate(testX, testy1h, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27e3c5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181991577148"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainacc_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d58c3e71-830a-477f-a360-a6394738dc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(100)\n",
    "rf.fit(trainX2D, trainy)\n",
    "y_pred = rf.predict(testX2D)\n",
    "accuracy_score(testy, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "008e26d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9454545454545454"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='linear') \n",
    "clf.fit(trainX2D, trainy) \n",
    "clf_pred = clf.predict(testX2D)\n",
    "accuracy_score(testy, clf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d240217-61b3-42c4-8791-d32ea09f2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadOne(featureList):\n",
    "    newFeatureList = [] # list of data per datatypes for angualrData, velocity data\n",
    "    for sampleList in featureList: # loops through angularV, linearV, accelerations etcs...\n",
    "        newSampleList = [] # list of all sampels for that dataType\n",
    "        for sample in sampleList:\n",
    "            if sample.shape[0] >= lengthFinal:\n",
    "                print(\"Larger than allowed,\", sample.shape)\n",
    "                sample = sample[:lengthFinal-sample.shape[0],:]\n",
    "            zerosLeft = (lengthFinal - sample.shape[0]) // 2\n",
    "            zerosRight = (lengthFinal - sample.shape[0] - zerosLeft)\n",
    "            # zerosLeft = 0\n",
    "            # zerosRight = (lengthFinal - sample.shape[0])\n",
    "            # print(sample.shape, \"->\", np.pad(sample, ((zerosLeft, zerosRight), (0, 0)), 'constant').shape)\n",
    "            # (53, 3) -> (188, 3)\n",
    "            \n",
    "            paddedSample = np.pad(sample, ((zerosLeft, zerosRight), (0, 0)), 'constant')\n",
    "            # print(sample.shape, '->', paddedSample.shape)\n",
    "            \n",
    "            newSampleList.append(paddedSample)\n",
    "        newFeatureList.append(np.array(newSampleList))\n",
    "         \n",
    "    combinedData = np.dstack(newFeatureList)\n",
    "    print(\"dstacked:\", combinedData.shape)\n",
    "\n",
    "    # combinedData = combinedData[:minSampleSize, ] # chop of so all labels are balanced\n",
    "    \n",
    "    return combinedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8347381-84c1-4eb0-86d0-b726cdc6c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger than allowed, (96, 3)\n",
      "Larger than allowed, (96, 3)\n",
      "dstacked: (202, 96, 6)\n",
      "Testing data shape not seen (19, 96, 6)\n",
      "Priniting NN:  S3 accuracy: 1.0\n",
      "Priniting RF:  S3 accuracy: 1.0\n",
      "Priniting CLF:  S3 accuracy: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "tst = 'S3'\n",
    "swishData = zeroPadOne(DataSet[tst])\n",
    "swishData = swishData[minSampleSize:,]\n",
    "# np.random.shuffle(swishData)\n",
    "print(\"Testing data shape not seen\", swishData.shape)\n",
    "\n",
    "# swishDatastd = np.array([stdScaler.fit_transform(swishData[x,]) for x in range(swishData.shape[0])])\n",
    "# swishDatamm =  np.array([mmScaler.fit_transform(swishData[x,]) for x in range(swishData.shape[0])])\n",
    "# swishDatanorm = np.array([normalize(swishData[x,], norm='l1') for x in range(swishData.shape[0])])\n",
    "\n",
    "swishy = np.full((swishData.shape[0],), label.transform([tst]))\n",
    "swishy1h = to_categorical(swishy, n_outputs)\n",
    "\n",
    "_, NN_acc = modelNN.evaluate(swishData, swishy1h, batch_size=32, verbose=0)\n",
    "print(\"Priniting NN: \", tst, \"accuracy:\", NN_acc)\n",
    "\n",
    "swishData2D = swishData.reshape((swishData.shape[0], swishData.shape[1]*swishData.shape[2]))\n",
    "rf_pred = rf.predict(swishData2D)\n",
    "clf_pred = clf.predict(swishData2D)\n",
    "rf_acc = accuracy_score(swishy, rf_pred)\n",
    "clf_acc = accuracy_score(swishy, clf_pred)\n",
    "\n",
    "\n",
    "print(\"Priniting RF: \", tst, \"accuracy:\", rf_acc)\n",
    "print(\"Priniting CLF: \", tst, \"accuracy:\", clf_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba5ef9-0155-43f8-9a0e-c80be4c6ccd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5a95f43-7aa7-4420-95f2-5e9608dbb79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bestFuckingMLModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bestFuckingMLModel\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelNN.save('bestFuckingMLModel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47d8161a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('./')\n",
    "os.system('python -m tf2onnx.convert --saved-model bestFuckingMLModel --output bestFuckingModel.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2858d10-06ab-405f-9694-65347a41a02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b558ea3-586c-44e4-8a78-e16c0a4c6379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61fae4-99ec-4e52-82ed-b6f49f6272a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
