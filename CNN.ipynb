{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84a17760-e6d0-423b-91c8-ce62090b64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, Normalization, BatchNormalization, LayerNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "013edb5c-ae83-4c41-bd48-06c6cc447cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData3x(file):\n",
    "    value = []\n",
    "    temp = []\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if (line[0] != '('):\n",
    "                if temp: \n",
    "                    value.append(np.array(temp))\n",
    "                temp = []\n",
    "            else:\n",
    "                data = line[1:-2].split(',')\n",
    "                temp.append([float(data[0]), float(data[1]), float(data[2])])\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd8c4e70-c227-4c3b-a98e-7abc5223d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 -> 264\n",
      "S2 -> 183\n",
      "S3 -> 202\n"
     ]
    }
   ],
   "source": [
    "wL = readData3x(\"hundred_right_lines/ControllerAngularVelocity.txt\")\n",
    "vL = readData3x(\"hundred_right_lines/ControllerVelocity.txt\")\n",
    "\n",
    "wI = readData3x(\"hundredInfinity/ControllerAngularVelocity.txt\")\n",
    "vI = readData3x(\"hundredInfinity/ControllerVelocity.txt\")\n",
    "\n",
    "wC = readData3x(\"hundredrightcircles/ControllerAngularVelocity.txt\")\n",
    "vC = readData3x(\"hundredrightcircles/ControllerVelocity.txt\")\n",
    "\n",
    "wS1 = readData3x(\"spell1CWC/ControllerAngularVelocity.txt\")\n",
    "vS1 = readData3x(\"spell1CWC/ControllerVelocity.txt\")\n",
    "\n",
    "wS2 = readData3x(\"spell2line/ControllerAngularVelocity.txt\")\n",
    "vS2 = readData3x(\"spell2line/ControllerVelocity.txt\")\n",
    "\n",
    "wS3 = readData3x(\"spell3/ControllerAngularVelocity.txt\")\n",
    "vS3 = readData3x(\"spell3/ControllerVelocity.txt\")\n",
    "            # This order of w first then v is important\n",
    "# DataSet = {'line': [wL, vL], 'infinity': [wI, vI], 'circle': [wC, vC]}\n",
    "# lenghtestlist = [wL, wI, wC]\n",
    "\n",
    "DataSet = {'S1': [wS1, vS1], 'S2': [wS2, vS2], 'S3': [wS3, vS3]}\n",
    "lenghtestlist = [v[0] for v in DataSet.values()]\n",
    "for label, data in DataSet.items():\n",
    "    print(label, \"->\", len(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba6032ba-01fd-4732-95fb-e1a811324df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of each 2D sample 96\n",
      "minimum samples per category are 183\n"
     ]
    }
   ],
   "source": [
    "longestLengths = []\n",
    "for type in lenghtestlist:\n",
    "    longestLengths.append(max([x.shape[0] for x in type]))\n",
    "\n",
    "lengthFinal = max(longestLengths)\n",
    "print(\"length of each 2D sample\", lengthFinal)\n",
    "\n",
    "minSampleSize = min([len(i[0]) for i in DataSet.values()])\n",
    "print(\"minimum samples per category are\", minSampleSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "255929a3-aa35-4d27-b623-5d14565bdefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 (264, 96, 3)\n",
      "S1 (264, 96, 3)\n",
      "dstacked: (264, 96, 6)\n",
      "chopped to min sample size: (183, 96, 6)\n",
      "\n",
      "S2 (183, 96, 3)\n",
      "S2 (183, 96, 3)\n",
      "dstacked: (183, 96, 6)\n",
      "chopped to min sample size: (183, 96, 6)\n",
      "\n",
      "S3 (202, 96, 3)\n",
      "S3 (202, 96, 3)\n",
      "dstacked: (202, 96, 6)\n",
      "chopped to min sample size: (183, 96, 6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDataSet = []\n",
    "y = []\n",
    "for label, dataType in DataSet.items():\n",
    "    dataTypeMatriciesLists = [] # list of data per datatypes for angualrData, velocity data\n",
    "    for dataMatrixList in dataType: # loops through angularV, linearV, accelerations etcs...\n",
    "        newDataMatrixList = [] # list of all sampels for that dataType\n",
    "        for dataMatrix in dataMatrixList:\n",
    "            # zero padding on both sides\n",
    "            zerosLeft = (lengthFinal - dataMatrix.shape[0]) // 2\n",
    "            zerosRight = (lengthFinal - dataMatrix.shape[0] - zerosLeft)\n",
    "            # zero pad right side\n",
    "            # zerosLeft = 0\n",
    "            # zerosRight = lengthFinal - dataMatrix.shape[0]\n",
    "            # zero padding only on right size\n",
    "            newDataMatrix = np.pad(dataMatrix, ((zerosLeft, zerosRight), (0, 0)), 'constant')\n",
    "            # print(dataMatrix.shape, '->', newDataMatrix.shape)\n",
    "            \n",
    "            newDataMatrixList.append(newDataMatrix)\n",
    "        dataTypeMatriciesLists.append(np.array(newDataMatrixList))\n",
    "        \n",
    "    [print(label,i.shape) for i in dataTypeMatriciesLists]\n",
    "    \n",
    "    combinedData = np.dstack(dataTypeMatriciesLists)\n",
    "    print(\"dstacked:\", combinedData.shape)\n",
    "    \n",
    "    combinedData = combinedData[:minSampleSize, ] # chop of so all labels are balanced\n",
    "    \n",
    "    print(\"chopped to min sample size:\", combinedData.shape, end=\"\\n\\n\")\n",
    "    newDataSet.append(combinedData)\n",
    "    y.append(np.full((combinedData.shape[0], 1), label))\n",
    "    # print(\"label dim:\", y[-1].shape, end=\"\\n\\n\")\n",
    "    \n",
    "newDataSet = np.vstack(newDataSet)\n",
    "DataSety = np.vstack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e806e9c-0c88-4dff-8e5d-494cd9e88691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((549, 96, 6), (549, 1))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDataSet.shape, DataSety.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b04ce664-492b-484e-a10e-c4bc31d8145b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Python3.11.1\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('S1', '->', 0), ('S2', '->', 1), ('S3', '->', 2)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdScaler = StandardScaler()\n",
    "mmScaler = MinMaxScaler()\n",
    "stdX = np.array([stdScaler.fit_transform(newDataSet[x,]) for x in range(newDataSet.shape[0])])\n",
    "# mmX =  np.array([mmScaler.fit_transform(newDataSet[x,]) for x in range(newDataSet.shape[0])])\n",
    "normX = np.array([normalize(newDataSet[x,], norm='l1') for x in range(newDataSet.shape[0])])\n",
    "\n",
    "label = LabelEncoder()\n",
    "DataSety = label.fit_transform(DataSety)\n",
    "\n",
    "[(label.classes_[i], '->', i) for i in range(len(label.classes_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7328054-b5f0-415a-8c4a-cb0c7b22e3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c781327-aa13-4070-aa3e-aa4ed4f6a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (494, 96, 6) test shape (55, 96, 6)\n",
      "Y train shape: (494,) Y test shape (55,)\n",
      "one hot train shape:-> (494, 3) one hot test shape:-> (55, 3)\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(newDataSet, DataSety, test_size = 0.1, random_state = 30, stratify = DataSety)\n",
    "trainX2D = trainX.reshape((trainX.shape[0], trainX.shape[1]*trainX.shape[2]))\n",
    "testX2D  = testX.reshape((testX.shape[0], testX.shape[1]*testX.shape[2]))\n",
    "\n",
    "print(\"train shape:\", trainX.shape, \"test shape\", testX.shape)\n",
    "print(\"Y train shape:\", trainy.shape, \"Y test shape\", testy.shape)\n",
    "\n",
    "# one hot encode y\n",
    "trainy1h = to_categorical(trainy)\n",
    "testy1h = to_categorical(testy)\n",
    "print(\"one hot train shape:->\", trainy1h.shape, \"one hot test shape:->\", testy1h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf38210b-3a75-4a8b-8948-920af72794e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy1h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cff78652-ec6a-4c66-b266-469f287ee6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy1h.shape[1]\n",
    "modelCNN = Sequential()\n",
    "modelCNN.add(Normalization(input_shape=(n_timesteps,n_features), name='input'))\n",
    "modelCNN.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "modelCNN.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "modelCNN.add(Dropout(0.5))\n",
    "modelCNN.add(MaxPooling1D(pool_size=2))\n",
    "modelCNN.add(Flatten())\n",
    "modelCNN.add(Dense(100, activation='relu'))\n",
    "modelCNN.add(Dense(n_outputs, activation='softmax', name='output'))\n",
    "modelCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d66ddc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Normalization)       (None, 96, 6)             13        \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 94, 64)            1216      \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 92, 64)            12352     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 92, 64)            0         \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, 46, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2944)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               294500    \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 308384 (1.18 MB)\n",
      "Trainable params: 308371 (1.18 MB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0af7c236-63a0-4ff7-b58f-19862db21559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 2s 12ms/step - loss: 0.4709 - accuracy: 0.8279\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0653 - accuracy: 0.9777\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0155 - accuracy: 0.9939\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0054 - accuracy: 0.9980\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 9.0425e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "modelCNN.fit(trainX, trainy1h, epochs=10, batch_size=32, verbose=1)\n",
    "_, accuracy = modelCNN.evaluate(testX, testy1h, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27e3c5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818181991577148"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d58c3e71-830a-477f-a360-a6394738dc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(100)\n",
    "rf.fit(trainX2D, trainy)\n",
    "y_pred = rf.predict(testX2D)\n",
    "accuracy_score(testy, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "008e26d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9454545454545454"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='linear') \n",
    "clf.fit(trainX2D, trainy) \n",
    "clf_pred = clf.predict(testX2D)\n",
    "accuracy_score(testy, clf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d240217-61b3-42c4-8791-d32ea09f2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manyof1cat(DataList):\n",
    "    dataTypeMatriciesLists = [] # list of data per datatypes for angualrData, velocity data\n",
    "    for dataMatrixList in DataList: # loops through angularV, linearV, accelerations etcs...\n",
    "        newDataMatrixList = [] # list of all sampels for that dataType\n",
    "        for dataMatrix in dataMatrixList:\n",
    "            if dataMatrix.shape[0] >= lengthFinal:\n",
    "                print(\"Larger than allowed,\", dataMatrix.shape)\n",
    "                dataMatrix = dataMatrix[:lengthFinal-dataMatrix.shape[0],:]\n",
    "            zerosLeft = (lengthFinal - dataMatrix.shape[0]) // 2\n",
    "            zerosRight = (lengthFinal - dataMatrix.shape[0] - zerosLeft)\n",
    "            # zerosLeft = 0\n",
    "            # zerosRight = (lengthFinal - dataMatrix.shape[0])\n",
    "            # print(dataMatrix.shape, \"->\", np.pad(dataMatrix, ((zerosLeft, zerosRight), (0, 0)), 'constant').shape)\n",
    "            # (53, 3) -> (188, 3)\n",
    "            \n",
    "            newDataMatrix = np.pad(dataMatrix, ((zerosLeft, zerosRight), (0, 0)), 'constant')\n",
    "            # print(dataMatrix.shape, '->', newDataMatrix.shape)\n",
    "            \n",
    "            newDataMatrixList.append(newDataMatrix)\n",
    "        dataTypeMatriciesLists.append(np.array(newDataMatrixList))\n",
    "         \n",
    "    combinedData = np.dstack(dataTypeMatriciesLists)\n",
    "    print(\"dstacked:\", combinedData.shape)\n",
    "\n",
    "    # combinedData = combinedData[:minSampleSize, ] # chop of so all labels are balanced\n",
    "    \n",
    "    return combinedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8347381-84c1-4eb0-86d0-b726cdc6c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger than allowed, (96, 3)\n",
      "Larger than allowed, (96, 3)\n",
      "dstacked: (202, 96, 6)\n",
      "Testing data shape not seen (19, 96, 6)\n",
      "Priniting CNN:  S3 accuracy: 1.0\n",
      "Priniting RF:  S3 accuracy: 0.9473684210526315\n",
      "Priniting CLF:  S3 accuracy: 0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "tst = 'S3'\n",
    "swishData = manyof1cat(DataSet[tst])\n",
    "swishData = swishData[minSampleSize:,]\n",
    "# np.random.shuffle(swishData)\n",
    "print(\"Testing data shape not seen\", swishData.shape)\n",
    "\n",
    "swishDatastd = np.array([stdScaler.fit_transform(swishData[x,]) for x in range(swishData.shape[0])])\n",
    "# swishDatamm =  np.array([mmScaler.fit_transform(swishData[x,]) for x in range(swishData.shape[0])])\n",
    "swishDatanorm = np.array([normalize(swishData[x,], norm='l1') for x in range(swishData.shape[0])])\n",
    "\n",
    "swishy = np.full((swishData.shape[0],), label.transform([tst]))\n",
    "swishy1h = to_categorical(swishy, n_outputs)\n",
    "\n",
    "_, accCNN = modelCNN.evaluate(swishDatastd, swishy1h, batch_size=32, verbose=0)\n",
    "print(\"Priniting CNN: \", tst, \"accuracy:\", accCNN)\n",
    "\n",
    "swishDatastd2D = swishDatastd.reshape((swishDatastd.shape[0], swishDatastd.shape[1]*swishDatastd.shape[2]))\n",
    "y_pred = rf.predict(swishDatastd2D)\n",
    "clf_pred = clf.predict(swishDatastd2D)\n",
    "rf_acc = accuracy_score(swishy, y_pred)\n",
    "clf_acc = accuracy_score(swishy, clf_pred)\n",
    "\n",
    "\n",
    "print(\"Priniting RF: \", tst, \"accuracy:\", rf_acc)\n",
    "print(\"Priniting CLF: \", tst, \"accuracy:\", clf_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba5ef9-0155-43f8-9a0e-c80be4c6ccd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5a95f43-7aa7-4420-95f2-5e9608dbb79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bestFuckingMLModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bestFuckingMLModel\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelCNN.save('bestFuckingMLModel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "47d8161a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('./')\n",
    "os.system('python -m tf2onnx.convert --saved-model bestFuckingMLModel --output bestFuckingModel.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2858d10-06ab-405f-9694-65347a41a02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b558ea3-586c-44e4-8a78-e16c0a4c6379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61fae4-99ec-4e52-82ed-b6f49f6272a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
